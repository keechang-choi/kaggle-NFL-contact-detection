{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/playground',\n",
       " '/opt/conda/lib/python310.zip',\n",
       " '/opt/conda/lib/python3.10',\n",
       " '/opt/conda/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/docker_user/.local/lib/python3.10/site-packages',\n",
       " '/opt/conda/lib/python3.10/site-packages',\n",
       " '/opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import argparse\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import sys\n",
    "import pandas as pd\n",
    "# 일단 __init__.py 추가해서 이렇게 해놨음.\n",
    "#BASE_PATH='/workspaces/kaggle-NFL-contact-detection'\n",
    "BASE_PATH='/workspace'\n",
    "\n",
    "sys.path.append(os.path.join(BASE_PATH, \"src\"))\n",
    "\n",
    "# timm as third-party\n",
    "sys.path.append(os.path.join(BASE_PATH, 'third-party/pytorch-image-models'))\n",
    "\n",
    "from config import CFG\n",
    "from factory.dataset_factory import DataSetFactory\n",
    "from factory.lightning_module_factory import LightningModuleFactory\n",
    "from train import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G포함과 G미포함 인 경우로 나눠서 test dataset을 구성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, Subset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import subprocess\n",
    "import torch\n",
    "import gc\n",
    "from typing import Tuple, List, Optional\n",
    "import pickle\n",
    "\n",
    "from config import CFG\n",
    "\n",
    "\n",
    "class CNN25SingleGroundDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, preprocess_result_dir, feature_cols, video2frames, aug, mode='train'):\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        # kaggle read only dir 문제로, 임시 생성 dir 별도 지정.\n",
    "        self.preprocess_result_dir = preprocess_result_dir\n",
    "        self.frame = df.frame.values\n",
    "        self.feature = df[feature_cols].fillna(-1).values\n",
    "        self.players = df[['nfl_player_id_1', 'nfl_player_id_2']].values\n",
    "        self.game_play = df.game_play.values\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "\n",
    "        self.video2frames = video2frames\n",
    "        self.image_size = 224\n",
    "        os.makedirs(self.preprocess_result_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # @lru_cache(1024)\n",
    "    # def read_img(self, path):\n",
    "    #     return cv2.imread(path, 0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = CFG[\"window\"]  # 24\n",
    "        frame = self.frame[idx]\n",
    "\n",
    "        # TODO: 이 부분 의미 잘 모르겠음. (기존코드)\n",
    "        # frame 값은 tracking data (10Hz)를 변환해서 얻은 값인데,\n",
    "        # 이 값에 해당하는 정확한 frame 값이 helmet data에 없을 수 있음\n",
    "        # if self.mode in [\"fit\", \"validate\", \"test\"]:\n",
    "        #     frame = frame + random.randint(-6, 6)\n",
    "\n",
    "        players = []\n",
    "        for p in self.players[idx]:\n",
    "            if p == 'G':\n",
    "                players.append(p)\n",
    "            else:\n",
    "                players.append(str(p))\n",
    "\n",
    "        imgs = []\n",
    "        for view in ['Endzone', 'Sideline']:\n",
    "            video = self.game_play[idx] + f'_{view}.mp4'\n",
    "            bboxes = []\n",
    "            query_game_play = self.game_play[idx]\n",
    "\n",
    "            query_res = self.df.query(f\"game_play == '{query_game_play}' and \"\n",
    "                                      f\"nfl_player_id_1 == '{players[0]}' and \"\n",
    "                                      f\"nfl_player_id_2 == '{players[1]}' and \"\n",
    "                                      f\"frame == {frame}\"\n",
    "                                     )\n",
    "            # bbox에 각 선수, view에 대한 frame의 window 평균 헬멧정보를 넣는다.\n",
    "            for i in range(2):\n",
    "\n",
    "                is_query_valid = False\n",
    "                if len(query_res) == 1:\n",
    "                    x = query_res.iloc[0][f\"{view}_left_{i+1}\"]\n",
    "                    w = query_res.iloc[0][f\"{view}_width_{i+1}\"]\n",
    "                    y = query_res.iloc[0][f\"{view}_top_{i+1}\"]\n",
    "                    h = query_res.iloc[0][f\"{view}_height_{i+1}\"]\n",
    "                    if all(not np.isnan(value) for value in [x, w, y, h]):\n",
    "                        is_query_valid = True\n",
    "                \n",
    "                if is_query_valid:\n",
    "                    bboxes.append([x, w, y, h])\n",
    "                else:\n",
    "                    bboxes.append([np.nan, np.nan, np.nan, np.nan])\n",
    "            bboxes = pd.DataFrame(bboxes).interpolate(\n",
    "                limit_direction='both').values\n",
    "            bboxes = bboxes[::4]\n",
    "\n",
    "            if bboxes.sum() > 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                # 보통 트랙킹 데이터에서 가져온 선수가 해당 프레임에서 안잡힌 경우.\n",
    "                flag = 0\n",
    "                print(\n",
    "                    f\"flag: {flag}-{len(query_res)} , idx: {idx}, video: {video}, frame: {frame}, player: {players}\")\n",
    "# 0.03s\n",
    "\n",
    "            # for i, f in enumerate(range(frame-window, frame+window+1, 4)):\n",
    "            for i, f in enumerate(range(frame, frame+1, 4)):\n",
    "                img_new = np.zeros(\n",
    "                    (self.image_size, self.image_size, 3), dtype=np.float32)\n",
    "                if f > self.video2frames[video]:\n",
    "                    print(\n",
    "                        f\"flag: {flag}, frame: {f}, video frame: {self.video2frames[video]}\")\n",
    "                if flag == 1 and f <= self.video2frames[video]:\n",
    "                    try:\n",
    "                        img = cv2.imread(\n",
    "                            os.path.join(self.preprocess_result_dir, f\"frames/{video}_{f:04d}.jpg\"), cv2.IMREAD_COLOR)\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        # \n",
    "                        \n",
    "                        x, w, y, h = bboxes[i]\n",
    "                        # print(f\"img helmet size: {w} x {h}\")\n",
    "                        # 10~60 정도? helmet size\n",
    "                        # 헬멧 크기의 10배 정도로 자름.\n",
    "                        crop_size = int((max(w, h)*10))\n",
    "                        # crop_size = 256\n",
    "                        img_tmp = np.zeros(\n",
    "                            (crop_size, crop_size, 3), dtype=np.float32)\n",
    "                        # print(crop_size)\n",
    "                        crop_half = crop_size // 2\n",
    "                        crop_start_y = max(int(y+h/2)-crop_half, 0)\n",
    "                        crop_end_y = min(int(y+h/2)+crop_half, img.shape[0])\n",
    "\n",
    "                        crop_start_x = max(int(x+w/2)-crop_half, 0)\n",
    "                        crop_end_x = min(int(x+w/2)+crop_half, img.shape[1])\n",
    "                        # crop할 크기에 따라 원본 프레임에서 이미지 잘라냄.\n",
    "                        img = img[crop_start_y:crop_end_y,\n",
    "                                  crop_start_x:crop_end_x,\n",
    "                                  :]\n",
    "                        # 자른 후 정규화 및 transform을 추가한다.\n",
    "                        img = self.aug(image=img)[\"image\"]\n",
    "                        \n",
    "\n",
    "                        # 이미지 가장자리에서 crop 크기가 안나오는 경우 있음.\n",
    "                        offset_y = (crop_size - img.shape[0]) // 2\n",
    "                        offset_x = (crop_size - img.shape[1]) // 2\n",
    "                        # resize 하기 전에 zero padding 사용해서 중앙으로 옮김.\n",
    "                        img_tmp[offset_y: offset_y+img.shape[0],\n",
    "                                offset_x: offset_x+img.shape[1],\n",
    "                                :] = img\n",
    "\n",
    "                        # resize를 통해 CNN에 들어갈 사이즈로 맞춰줌.\n",
    "                        img_tmp = cv2.resize(\n",
    "                            img_tmp, (self.image_size, self.image_size))\n",
    "                        img_new[:img_tmp.shape[0], :img_tmp.shape[1], :] = img_tmp\n",
    "                    except Exception as e:\n",
    "                        print(os.path.join(self.preprocess_result_dir,\n",
    "                              f\"frames/{video}_{f:04d}.jpg\"))\n",
    "                        print(os.path.exists(os.path.join(\n",
    "                            self.preprocess_result_dir, f\"frames/{video}_{f:04d}.jpg\")))\n",
    "                        print(f\"box: {(x,y)}, {(w,h)}\")\n",
    "                        print(f\"img is None: {img is None}\")\n",
    "                        print(f\"img shape: {img.shape}\")\n",
    "                        print(e)\n",
    "                        raise e\n",
    "                \n",
    "                imgs.append(img_new)\n",
    "# 0.06s\n",
    "\n",
    "        feature = np.float32(self.feature[idx])\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        label = np.float32(self.df.contact.values[idx])\n",
    "\n",
    "        return imgs, feature, label\n",
    "\n",
    "\n",
    "class CNN25SingleGroundDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\", preprocess_result_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.preprocess_result_dir = preprocess_result_dir\n",
    "\n",
    "        self.train_aug = A.Compose([\n",
    "            # Bright and Contrast가 의미 있는지 모르겠으나, normalize 안해주면 에러발생.\n",
    "            A.ToFloat(max_value=255),\n",
    "            A.HorizontalFlip(p=0.5),  # 숫자 뒤집어 짐\n",
    "            A.ShiftScaleRotate(p=0.5, rotate_limit=5),  # 45도는 너무 큼.\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225),\n",
    "                        max_pixel_value=1.0),\n",
    "        ])\n",
    "\n",
    "        self.valid_aug = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225),\n",
    "                        max_pixel_value=255.0)\n",
    "        ])\n",
    "        self.use_cols = [\n",
    "            'x_position', 'y_position', 'speed', 'distance',\n",
    "            'direction', 'orientation', 'acceleration', 'sa'\n",
    "        ]\n",
    "        # TODO: 이 부분 hard-coding 된 것 개선.\n",
    "        self.feature_cols: List[str] = [\"distance\", \"G_flug\"]\n",
    "        for col in self.use_cols:\n",
    "            self.feature_cols.append(col + \"_1\")\n",
    "            self.feature_cols.append(col + \"_2\")\n",
    "\n",
    "        self.dataset_test = None\n",
    "        self.dataset_train = None\n",
    "        self.dataset_valid = None\n",
    "        self.dataset_pred = None\n",
    "\n",
    "    def expand_contact_id(self, df):\n",
    "        \"\"\"\n",
    "        Splits out contact_id into seperate columns.\n",
    "        \"\"\"\n",
    "        df[\"game_play\"] = df[\"contact_id\"].str[:12]\n",
    "        df[\"step\"] = df[\"contact_id\"].str.split(\"_\").str[-3].astype(\"int\")\n",
    "        df[\"nfl_player_id_1\"] = df[\"contact_id\"].str.split(\"_\").str[-2]\n",
    "        df[\"nfl_player_id_2\"] = df[\"contact_id\"].str.split(\"_\").str[-1]\n",
    "        return df\n",
    "\n",
    "    def create_features(self, df, tr_tracking, merge_col=\"step\", use_cols=[\"x_position\", \"y_position\"]):\n",
    "        output_cols = []\n",
    "        df_combo = (\n",
    "            df.astype({\"nfl_player_id_1\": \"str\"})\n",
    "            .merge(\n",
    "                tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
    "                    [\"game_play\", merge_col, \"nfl_player_id\",] + use_cols\n",
    "                ],\n",
    "                left_on=[\"game_play\", merge_col, \"nfl_player_id_1\"],\n",
    "                right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .rename(columns={c: c+\"_1\" for c in use_cols})\n",
    "            .drop(\"nfl_player_id\", axis=1)\n",
    "            .merge(\n",
    "                tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
    "                    [\"game_play\", merge_col, \"nfl_player_id\"] + use_cols\n",
    "                ],\n",
    "                left_on=[\"game_play\", merge_col, \"nfl_player_id_2\"],\n",
    "                right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(\"nfl_player_id\", axis=1)\n",
    "            .rename(columns={c: c+\"_2\" for c in use_cols})\n",
    "            .sort_values([\"game_play\", merge_col, \"nfl_player_id_1\", \"nfl_player_id_2\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        output_cols += [c+\"_1\" for c in use_cols]\n",
    "        output_cols += [c+\"_2\" for c in use_cols]\n",
    "\n",
    "        if (\"x_position\" in use_cols) & (\"y_position\" in use_cols):\n",
    "            index = df_combo['x_position_2'].notnull()\n",
    "\n",
    "            distance_arr = np.full(len(index), np.nan)\n",
    "            tmp_distance_arr = np.sqrt(\n",
    "                np.square(df_combo.loc[index, \"x_position_1\"] -\n",
    "                          df_combo.loc[index, \"x_position_2\"])\n",
    "                + np.square(df_combo.loc[index, \"y_position_1\"] -\n",
    "                            df_combo.loc[index, \"y_position_2\"])\n",
    "            )\n",
    "\n",
    "            distance_arr[index] = tmp_distance_arr\n",
    "            df_combo['distance'] = distance_arr\n",
    "            output_cols += [\"distance\"]\n",
    "\n",
    "        df_combo['G_flug'] = (df_combo['nfl_player_id_2'] == \"G\")\n",
    "        output_cols += [\"G_flug\"]\n",
    "        return df_combo, output_cols\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # 다운로드 및 전처리\n",
    "        # NOTE: single core로 실행됨.\n",
    "        # https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html\n",
    "        # NOTE: state 저장하지 말고 disk에 저장해야함.\n",
    "        self.preprocess_dataset()\n",
    "        \n",
    "    def reindex_by_frame(self, df):\n",
    "        frames = np.arange(df.index.min()-CFG[\"window\"], df.index.max()+CFG[\"window\"])\n",
    "        game_play = df.iloc[0][\"game_play\"]\n",
    "        view = df.iloc[0][\"view\"]\n",
    "        nfl_player_id = df.iloc[0][\"nfl_player_id\"]\n",
    "        df = df.reindex(frames, fill_value=np.nan)\n",
    "        df[\"game_play\"] = df[\"game_play\"].fillna(game_play)\n",
    "        df[\"view\"] = df[\"view\"].fillna(view)\n",
    "        df[\"nfl_player_id\"] = df[\"nfl_player_id\"].fillna(nfl_player_id)\n",
    "        return df\n",
    "    \n",
    "    def preprocess_dataset(self):\n",
    "        # 데이터 전처리 후, 파일로 저장해놓는다.\n",
    "        # fit,validate,test 공통으로 한번, predict 한번씩만 실행되면 된다.\n",
    "        is_prediction = CFG[\"is_prediction\"]\n",
    "        run_type = \"test\" if is_prediction else \"train\"\n",
    "        frame_dir = os.path.join(self.preprocess_result_dir, \"frames\")\n",
    "        processed_meta_dir = os.path.join(self.preprocess_result_dir, run_type)\n",
    "\n",
    "        os.makedirs(frame_dir, exist_ok=True)\n",
    "        os.makedirs(processed_meta_dir, exist_ok=True)\n",
    "\n",
    "        print(\"====== [Preprocess] ======\")\n",
    "        print(f\"- is_prediction: {is_prediction}\")\n",
    "        print(f\"- run_type: {run_type}\")\n",
    "\n",
    "        print(\"------ [Loading Metadata] ------\")\n",
    "        df_helmets = pd.read_csv(os.path.join(\n",
    "            self.data_dir, f\"{run_type}_baseline_helmets.csv\"))\n",
    "        df_video_metadata = pd.read_csv(os.path.join(\n",
    "            self.data_dir, f\"{run_type}_video_metadata.csv\"))\n",
    "\n",
    "        print(\"------ [ffmpeg] ------\")\n",
    "        print(f\"ffmpeg frames {run_type}\")\n",
    "        for video in tqdm(df_helmets.video.unique()):\n",
    "            if os.path.isfile(os.path.join(frame_dir, video+\"_0001.jpg\")):\n",
    "                continue\n",
    "            if \"Endzone2\" not in video:\n",
    "                subprocess.call([\"ffmpeg\", \"-i\", os.path.join(self.data_dir, f\"{run_type}/{video}\"), \"-q:v\", \"2\", \"-f\", \"image2\", os.path.join(\n",
    "                    frame_dir, f\"{video}_%04d.jpg\"), \"-hide_banner\", \"-loglevel\", \"error\"])\n",
    "\n",
    "        print(\"------ [Mapping metadata] ------\")\n",
    "#         if CFG[\"reproduce_processed_data\"] or not os.path.exists(os.path.join(processed_meta_dir, \"video2helmets.pickle\")):\n",
    "#             print(\n",
    "#                 f\"Mapping video2helmets [size: {len(df_helmets.video.unique())}]\")\n",
    "#             video2helmets = {}\n",
    "#             df_helmets_new = df_helmets.set_index('video')\n",
    "#             for video in tqdm(df_helmets.video.unique()):\n",
    "#                 video2helmets[video] = df_helmets_new.loc[video].reset_index(\n",
    "#                     drop=True)\n",
    "#             with open(os.path.join(processed_meta_dir, \"video2helmets.pickle\"), \"wb\") as f:\n",
    "#                 pickle.dump(video2helmets, f)\n",
    "#             # 메모리 이슈\n",
    "#             del df_helmets, df_helmets_new\n",
    "#         else:\n",
    "#             print(f\"video2helemts already exists.. skip\")\n",
    "\n",
    "        if CFG[\"reproduce_processed_data\"] or not os.path.exists(os.path.join(processed_meta_dir, \"video2frames.pickle\")):\n",
    "            print(\n",
    "                f\"-- Mapping video2frames: [size: {len(df_video_metadata.game_play.unique())}]\")\n",
    "            video2frames = {}\n",
    "            for game_play in tqdm(df_video_metadata.game_play.unique()):\n",
    "                for view in ['Endzone', 'Sideline']:\n",
    "                    video = game_play + f'_{view}.mp4'\n",
    "                    video2frames[video] = max(list(map(\n",
    "                        lambda x: int(x.split('_')[-1].split('.')[0]),\n",
    "                        glob.glob(os.path.join(frame_dir, f'{video}*')\n",
    "                                  ))))\n",
    "                with open(os.path.join(processed_meta_dir, \"video2frames.pickle\"), \"wb\") as f:\n",
    "                    pickle.dump(video2frames, f)\n",
    "            # 메모리 이슈\n",
    "            del video2frames\n",
    "        else:\n",
    "            print(f\"video2frames already exists.. skip\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"------ [Preprocess helmet sensor data] ------\")\n",
    "        # CFG[\"reproduce_processed_data\"]\n",
    "        if CFG[\"reproduce_processed_data\"] or not os.path.exists(os.path.join(processed_meta_dir, \"df_filtered.csv\")):\n",
    "            df_tracking = pd.read_csv(os.path.join(\n",
    "                self.data_dir, f\"{run_type}_player_tracking.csv\"))\n",
    "\n",
    "            if is_prediction:\n",
    "                label_file_name = \"sample_submission.csv\"\n",
    "            else:\n",
    "                label_file_name = \"train_labels.csv\"\n",
    "\n",
    "            print(f\"- Expand contact id\")\n",
    "            labels = self.expand_contact_id(pd.read_csv(\n",
    "                os.path.join(self.data_dir, label_file_name)))\n",
    "            print(f\"- Create features\")\n",
    "            df_with_feature, _ = self.create_features(\n",
    "                labels, df_tracking, use_cols=self.use_cols)\n",
    "            df_filtered = df_with_feature.query(\n",
    "                'not distance>2').reset_index(drop=True)\n",
    "            df_filtered['frame'] = (\n",
    "                df_filtered['step']/10*59.94+5*59.94).astype('int')+1\n",
    "\n",
    "            # 메모리 이슈\n",
    "            del df_with_feature, labels, df_tracking\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"- Rolling helmet data\")\n",
    "            df_helmets[\"nfl_player_id\"] = df_helmets[\"nfl_player_id\"].astype(\"string\")\n",
    "            # helmet 데이터에 비어있는 frame을 nan으로 채운다.\n",
    "            df_reindexed_helmets = df_helmets[[\"game_play\",\"view\", \"frame\", \"nfl_player_id\", \"left\",\"width\", \"top\", \"height\"]]\\\n",
    "                .set_index(\"frame\")\\\n",
    "                .groupby([\"game_play\", \"view\", \"nfl_player_id\"], dropna=False)\\\n",
    "                .apply(self.reindex_by_frame)\\\n",
    "                .reset_index([\"game_play\", \"view\", \"nfl_player_id\"], drop=True)\n",
    "            \n",
    "            # helmet 데이터의 각 frame을 window 크기로 돌며 위치와 크기를 평균낸다.\n",
    "            df_rolled_helmets = df_reindexed_helmets.groupby([\"game_play\", \"view\", \"nfl_player_id\"], dropna=False)\\\n",
    "                .rolling(CFG[\"window\"], min_periods=1, center=True)\\\n",
    "                .mean()\\\n",
    "                .reset_index()\n",
    "            \n",
    "            df_rolled_helmets[\"nfl_player_id\"] = df_rolled_helmets[\"nfl_player_id\"].astype(\"string\")\n",
    "\n",
    "            # Endzone2 있는거 제거\n",
    "            df_rolled_helmets = df_rolled_helmets[df_rolled_helmets[\"view\"] != \"Endzone2\"]\n",
    "\n",
    "            # Endzone과 Sideline의 각 xwyh 를 분리한다.\n",
    "            df_rolled_helmets = df_rolled_helmets.pivot_table(values=[\"left\",\"width\", \"top\", \"height\"], index=[\"game_play\", \"nfl_player_id\", \"frame\"], columns=[\"view\"], aggfunc=\"first\").reset_index()\n",
    "\n",
    "            # 컬럼 이름을 다시 지정한다.\n",
    "            new_columns = []\n",
    "            for c in df_rolled_helmets.columns.to_flat_index():\n",
    "                if '' in c:\n",
    "                    new_columns.append(c[0])\n",
    "                else:\n",
    "                    new_columns.append(c[1]+\"_\"+c[0])\n",
    "            df_rolled_helmets.columns = new_columns\n",
    "            # 각 컬럼이름\n",
    "            rename_list = [\"nfl_player_id\", \"Endzone_left\", \"Endzone_width\", \"Endzone_top\", \"Endzone_height\", \"Sideline_left\", \"Sideline_width\", \"Sideline_top\", \"Sideline_height\"]\n",
    "\n",
    "            # 각 컬럼 이름에 \"_1\"과 \"_2\"를 추가해서 rename으로 사용한다.\n",
    "            for i in range(2):\n",
    "                rename_dict = {k:k+f\"_{i+1}\" for k in rename_list}\n",
    "                df_filtered[f\"nfl_player_id_{i+1}\"] = df_filtered[f\"nfl_player_id_{i+1}\"].astype(\"string\")\n",
    "                # player i+1에 대한 Enndzone, Sideline에서의 헬멧정보\n",
    "                df_filtered = df_filtered.merge(df_rolled_helmets.rename(columns=rename_dict), \n",
    "                    left_on=[\"game_play\", f\"nfl_player_id_{i+1}\", \"frame\"], \n",
    "                    right_on=[\"game_play\", f\"nfl_player_id_{i+1}\", \"frame\"],\n",
    "                    how=\"left\"\n",
    "                )\n",
    "\n",
    "            # contact=1인데 헬멧정보가 없는 경우를 확인\n",
    "            # print(df_filtered)\n",
    "            # print(df_filtered[df_filtered[\"contact\"] == 1 & (df_filtered[\"Endzone_left_1\"].isnull() | df_filtered[\"Sideline_left_1\"].isnull())])\n",
    "\n",
    "            # save preprocessed files to writable dir.\n",
    "            df_filtered.to_csv(os.path.join(\n",
    "                processed_meta_dir, \"df_filtered.csv\"), index=False)\n",
    "        else:\n",
    "            print(\"df_filtered already exists.. skip\")\n",
    "\n",
    "    def generate_dataset(self, stage: str) -> CNN25SingleGroundDataset:\n",
    "        # 학습 데이터 split을 수행한다.\n",
    "\n",
    "        print(f\"====== Generating dataset  ======\")\n",
    "        print(f\"- stage: {stage}\")\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            run_type = \"test\"\n",
    "        else:\n",
    "            run_type = \"train\"\n",
    "\n",
    "        processed_meta_dir = os.path.join(self.preprocess_result_dir, run_type)\n",
    "\n",
    "        print(f\"------ [Load metadata] ------\")\n",
    "        df_filtered = pd.read_csv(os.path.join(\n",
    "            processed_meta_dir, f\"df_filtered.csv\"),\n",
    "                                 dtype={\"nfl_player_id_1\":\"string\", \"nfl_player_id_2\":\"string\"})\n",
    "        # type string\n",
    "        \n",
    "#         with open(os.path.join(processed_meta_dir, \"video2helmets.pickle\"), \"rb\") as f:\n",
    "#             video2helmets = pickle.load(f)\n",
    "        with open(os.path.join(processed_meta_dir, \"video2frames.pickle\"), \"rb\") as f:\n",
    "            video2frames = pickle.load(f)\n",
    "\n",
    "        if stage in [\"fit\", \"validate\", \"test\"]:\n",
    "            # TODO: config 바로 이용하는게낫나, args로 넘기는게 낫나\n",
    "            game_play_arr: np.ndarray\n",
    "            game_play_arr = df_filtered.groupby(\n",
    "                \"game_play\")[\"game_play\"].first().to_numpy()\n",
    "            # 사용할 game_play 수를 줄여준다.\n",
    "            game_play_arr = game_play_arr[:CFG[\"num_train_video\"]]\n",
    "\n",
    "            train_video_size = int(len(game_play_arr) * 0.8)\n",
    "            valid_video_size = int(len(game_play_arr) * 0.1)\n",
    "            test_video_size = len(game_play_arr) - \\\n",
    "                (train_video_size + valid_video_size)\n",
    "            # TODO: seed 고정을 함수 밖으로 옮기는게 나을지\n",
    "            seed = torch.Generator().manual_seed(CFG[\"seed\"])\n",
    "            train_split, valid_split, test_split = random_split(\n",
    "                game_play_arr, [train_video_size, valid_video_size, test_video_size], generator=seed)\n",
    "\n",
    "            if stage == \"fit\":\n",
    "                game_play_filter_arr = game_play_arr[train_split.indices]\n",
    "            elif stage == \"validate\":\n",
    "                game_play_filter_arr = game_play_arr[valid_split.indices]\n",
    "            else:\n",
    "                # test\n",
    "                game_play_filter_arr = game_play_arr[test_split.indices]\n",
    "            print(\n",
    "                f\"-- num_videos: {len(game_play_filter_arr)}/{len(game_play_arr)}\")\n",
    "            df_filtered_dataset = df_filtered[df_filtered[\"game_play\"].isin(\n",
    "                game_play_filter_arr)]\n",
    "        else:\n",
    "            df_filtered_dataset = df_filtered\n",
    "\n",
    "        # NOTE: predict에서는 split 및 데이터 줄이는 과정이 필요없다.\n",
    "\n",
    "        print(f\"-- Label count {stage}: \")\n",
    "        print(df_filtered_dataset.groupby(\"contact\")[\"contact\"].count())\n",
    "\n",
    "        if stage in [\"fit\"]:\n",
    "            aug = self.train_aug\n",
    "        else:\n",
    "            aug = self.valid_aug\n",
    "\n",
    "        df_filtered_dataset = df_filtered_dataset.query(\n",
    "            'G_flug == True').reset_index(drop=True)\n",
    "        df_filtered_dataset = df_filtered_dataset.query(\n",
    "            'contact == 1').reset_index(drop=True)\n",
    "\n",
    "        dataset = CNN25SingleGroundDataset(\n",
    "            df=df_filtered_dataset,\n",
    "            data_dir=self.data_dir,\n",
    "            preprocess_result_dir=self.preprocess_result_dir,\n",
    "            feature_cols=self.feature_cols,\n",
    "            video2frames=video2frames,\n",
    "            aug=aug,\n",
    "            mode=stage)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # dataset 생성\n",
    "        # stage 는 fit/validate/test/predict 중 하나임.\n",
    "        # train_ 데이터를 다시 train/validation/test로 나누고,\n",
    "        # test_ 데이터는 predict에 사용함.\n",
    "\n",
    "        # https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html\n",
    "        # 데이터셋 생성\n",
    "        print(\"------ [Setup dataset] ------\")\n",
    "        if stage == \"fit\":\n",
    "            self.dataset_train = self.generate_dataset(\"fit\")\n",
    "            print(f\"- fit_dataset_size: {len(self.dataset_train)}\")\n",
    "            self.dataset_valid = self.generate_dataset(\"validate\")\n",
    "            print(f\"- validate_dataset_size: {len(self.dataset_valid)}\")\n",
    "        elif stage == \"validate\":\n",
    "            self.dataset_valid = self.generate_dataset(\"validate\")\n",
    "            print(f\"- {stage}_dataset_size: {len(self.dataset_valid)}\")\n",
    "        elif stage == \"test\":\n",
    "            self.dataset_test = self.generate_dataset(\"test\")\n",
    "            print(f\"- {stage}_dataset_size: {len(self.dataset_test)}\")\n",
    "        elif stage == \"predict\":\n",
    "            self.dataset_pred = self.generate_dataset(\"predict\")\n",
    "            print(f\"- {stage}_dataset_size: {len(self.dataset_pred)}\")\n",
    "        else:\n",
    "            raise TypeError(\"stage error.\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, batch_size=CFG[\"batch_size\"], num_workers=CFG[\"num_workers\"], shuffle=True, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_valid, batch_size=CFG[\"batch_size\"], num_workers=CFG[\"num_workers\"])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, batch_size=CFG[\"batch_size\"], num_workers=CFG[\"num_workers\"])\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.dataset_pred, batch_size=CFG[\"batch_size\"], num_workers=CFG[\"num_workers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: data dir train/pred 구분.\n",
    "CFG[\"dataset_params\"][\"data_dir\"] = os.path.join(BASE_PATH, \"data\")\n",
    "CFG[\"dataset_params\"][\"preprocess_result_dir\"] = os.path.join(BASE_PATH, \"data/processed\")\n",
    "CFG[\"is_prediction\"] = False\n",
    "CFG[\"reproduce_processed_data\"] = False\n",
    "CFG[\"threshold\"] = 0.344\n",
    "CFG[\"num_workers\"] = 4\n",
    "CFG[\"window\"]=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(CFG['seed'])\n",
    "device_str = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device_str = \"cuda\"\n",
    "device_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = os.path.join(\n",
    "    BASE_PATH, \n",
    "    \"epoch-8-step-198180-012222.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = f\"{CFG['model_name']}-{CFG['model_version']}\"\n",
    "dataset_params = CFG['dataset_params']\n",
    "model_params = CFG['model_params']\n",
    "# data_module = DataSetFactory.get_dataset(name=model_name,\n",
    "#                                              params=dataset_params)\n",
    "data_module = CNN25SingleGroundDataModule(**dataset_params)\n",
    "lightning_module = LightningModuleFactory.get_lightning_module(name=model_name,\n",
    "                                                                load_path=\"\",\n",
    "                                                                params=model_params)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=CFG[\"epochs\"],\n",
    "                        accelerator=device_str,\n",
    "                        devices=1 if device_str != \"cpu\" else None,\n",
    "                        logger=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== [Preprocess] ======\n",
      "- is_prediction: False\n",
      "- run_type: train\n",
      "------ [Loading Metadata] ------\n",
      "------ [ffmpeg] ------\n",
      "ffmpeg frames train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 481/481 [00:00<00:00, 257657.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ [Mapping metadata] ------\n",
      "video2frames already exists.. skip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ [Preprocess helmet sensor data] ------\n",
      "df_filtered already exists.. skip\n",
      "------ [Setup dataset] ------\n",
      "====== Generating dataset  ======\n",
      "- stage: test\n",
      "------ [Load metadata] ------\n",
      "-- num_videos: 24/240\n",
      "-- Label count test: \n",
      "contact\n",
      "0    58681\n",
      "1     6696\n",
      "Name: contact, dtype: int64\n",
      "- test_dataset_size: 2006\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_module.prepare_data()\n",
    "data_module.setup(\"test\")\n",
    "dl = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "336\n",
      "249\n",
      "147\n",
      "1568\n",
      "2006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contact_id</th>\n",
       "      <th>game_play</th>\n",
       "      <th>datetime</th>\n",
       "      <th>step</th>\n",
       "      <th>nfl_player_id_1</th>\n",
       "      <th>nfl_player_id_2</th>\n",
       "      <th>contact</th>\n",
       "      <th>x_position_1</th>\n",
       "      <th>y_position_1</th>\n",
       "      <th>speed_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Endzone_width_1</th>\n",
       "      <th>Sideline_width_1</th>\n",
       "      <th>Endzone_height_2</th>\n",
       "      <th>Sideline_height_2</th>\n",
       "      <th>Endzone_left_2</th>\n",
       "      <th>Sideline_left_2</th>\n",
       "      <th>Endzone_top_2</th>\n",
       "      <th>Sideline_top_2</th>\n",
       "      <th>Endzone_width_2</th>\n",
       "      <th>Sideline_width_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58187_001383_72_38696_G</td>\n",
       "      <td>58187_001383</td>\n",
       "      <td>2020-09-20T17:56:22.500Z</td>\n",
       "      <td>72</td>\n",
       "      <td>38696</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>77.86</td>\n",
       "      <td>48.38</td>\n",
       "      <td>5.71</td>\n",
       "      <td>...</td>\n",
       "      <td>19.708333</td>\n",
       "      <td>9.444444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58187_001383_72_46087_G</td>\n",
       "      <td>58187_001383</td>\n",
       "      <td>2020-09-20T17:56:22.500Z</td>\n",
       "      <td>72</td>\n",
       "      <td>46087</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>77.40</td>\n",
       "      <td>47.97</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>23.916667</td>\n",
       "      <td>13.208333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58187_001383_73_38696_G</td>\n",
       "      <td>58187_001383</td>\n",
       "      <td>2020-09-20T17:56:22.600Z</td>\n",
       "      <td>73</td>\n",
       "      <td>38696</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>78.20</td>\n",
       "      <td>48.79</td>\n",
       "      <td>5.10</td>\n",
       "      <td>...</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>11.388889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58187_001383_73_46087_G</td>\n",
       "      <td>58187_001383</td>\n",
       "      <td>2020-09-20T17:56:22.600Z</td>\n",
       "      <td>73</td>\n",
       "      <td>46087</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>77.78</td>\n",
       "      <td>48.41</td>\n",
       "      <td>5.55</td>\n",
       "      <td>...</td>\n",
       "      <td>25.541667</td>\n",
       "      <td>14.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58187_001383_74_38696_G</td>\n",
       "      <td>58187_001383</td>\n",
       "      <td>2020-09-20T17:56:22.700Z</td>\n",
       "      <td>74</td>\n",
       "      <td>38696</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>78.50</td>\n",
       "      <td>49.13</td>\n",
       "      <td>4.22</td>\n",
       "      <td>...</td>\n",
       "      <td>23.708333</td>\n",
       "      <td>12.047619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>58565_001768_59_25511_G</td>\n",
       "      <td>58565_001768</td>\n",
       "      <td>2021-10-04T01:27:50.900Z</td>\n",
       "      <td>59</td>\n",
       "      <td>25511</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>37.00</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>14.727273</td>\n",
       "      <td>7.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>58565_001768_60_25511_G</td>\n",
       "      <td>58565_001768</td>\n",
       "      <td>2021-10-04T01:27:51.000Z</td>\n",
       "      <td>60</td>\n",
       "      <td>25511</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>37.01</td>\n",
       "      <td>21.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>58565_001768_61_25511_G</td>\n",
       "      <td>58565_001768</td>\n",
       "      <td>2021-10-04T01:27:51.100Z</td>\n",
       "      <td>61</td>\n",
       "      <td>25511</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>36.99</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>58565_001768_62_25511_G</td>\n",
       "      <td>58565_001768</td>\n",
       "      <td>2021-10-04T01:27:51.200Z</td>\n",
       "      <td>62</td>\n",
       "      <td>25511</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>36.96</td>\n",
       "      <td>21.05</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>58565_001768_63_25511_G</td>\n",
       "      <td>58565_001768</td>\n",
       "      <td>2021-10-04T01:27:51.300Z</td>\n",
       "      <td>63</td>\n",
       "      <td>25511</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>36.94</td>\n",
       "      <td>20.98</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1568 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   contact_id     game_play                  datetime  step  \\\n",
       "0     58187_001383_72_38696_G  58187_001383  2020-09-20T17:56:22.500Z    72   \n",
       "1     58187_001383_72_46087_G  58187_001383  2020-09-20T17:56:22.500Z    72   \n",
       "2     58187_001383_73_38696_G  58187_001383  2020-09-20T17:56:22.600Z    73   \n",
       "3     58187_001383_73_46087_G  58187_001383  2020-09-20T17:56:22.600Z    73   \n",
       "4     58187_001383_74_38696_G  58187_001383  2020-09-20T17:56:22.700Z    74   \n",
       "...                       ...           ...                       ...   ...   \n",
       "1946  58565_001768_59_25511_G  58565_001768  2021-10-04T01:27:50.900Z    59   \n",
       "1948  58565_001768_60_25511_G  58565_001768  2021-10-04T01:27:51.000Z    60   \n",
       "1950  58565_001768_61_25511_G  58565_001768  2021-10-04T01:27:51.100Z    61   \n",
       "1952  58565_001768_62_25511_G  58565_001768  2021-10-04T01:27:51.200Z    62   \n",
       "1954  58565_001768_63_25511_G  58565_001768  2021-10-04T01:27:51.300Z    63   \n",
       "\n",
       "     nfl_player_id_1 nfl_player_id_2  contact  x_position_1  y_position_1  \\\n",
       "0              38696               G        1         77.86         48.38   \n",
       "1              46087               G        1         77.40         47.97   \n",
       "2              38696               G        1         78.20         48.79   \n",
       "3              46087               G        1         77.78         48.41   \n",
       "4              38696               G        1         78.50         49.13   \n",
       "...              ...             ...      ...           ...           ...   \n",
       "1946           25511               G        1         37.00         21.22   \n",
       "1948           25511               G        1         37.01         21.15   \n",
       "1950           25511               G        1         36.99         21.11   \n",
       "1952           25511               G        1         36.96         21.05   \n",
       "1954           25511               G        1         36.94         20.98   \n",
       "\n",
       "      speed_1  ...  Endzone_width_1  Sideline_width_1  Endzone_height_2  \\\n",
       "0        5.71  ...        19.708333          9.444444               NaN   \n",
       "1        6.00  ...        23.916667         13.208333               NaN   \n",
       "2        5.10  ...        21.333333         11.388889               NaN   \n",
       "3        5.55  ...        25.541667         14.166667               NaN   \n",
       "4        4.22  ...        23.708333         12.047619               NaN   \n",
       "...       ...  ...              ...               ...               ...   \n",
       "1946     0.27  ...        14.727273          7.857143               NaN   \n",
       "1948     0.40  ...        14.583333          8.250000               NaN   \n",
       "1950     0.49  ...        13.600000          8.444444               NaN   \n",
       "1952     0.59  ...        10.000000          8.666667               NaN   \n",
       "1954     0.74  ...        13.000000          9.333333               NaN   \n",
       "\n",
       "      Sideline_height_2  Endzone_left_2  Sideline_left_2  Endzone_top_2  \\\n",
       "0                   NaN             NaN              NaN            NaN   \n",
       "1                   NaN             NaN              NaN            NaN   \n",
       "2                   NaN             NaN              NaN            NaN   \n",
       "3                   NaN             NaN              NaN            NaN   \n",
       "4                   NaN             NaN              NaN            NaN   \n",
       "...                 ...             ...              ...            ...   \n",
       "1946                NaN             NaN              NaN            NaN   \n",
       "1948                NaN             NaN              NaN            NaN   \n",
       "1950                NaN             NaN              NaN            NaN   \n",
       "1952                NaN             NaN              NaN            NaN   \n",
       "1954                NaN             NaN              NaN            NaN   \n",
       "\n",
       "      Sideline_top_2  Endzone_width_2  Sideline_width_2  \n",
       "0                NaN              NaN               NaN  \n",
       "1                NaN              NaN               NaN  \n",
       "2                NaN              NaN               NaN  \n",
       "3                NaN              NaN               NaN  \n",
       "4                NaN              NaN               NaN  \n",
       "...              ...              ...               ...  \n",
       "1946             NaN              NaN               NaN  \n",
       "1948             NaN              NaN               NaN  \n",
       "1950             NaN              NaN               NaN  \n",
       "1952             NaN              NaN               NaN  \n",
       "1954             NaN              NaN               NaN  \n",
       "\n",
       "[1568 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_module.dataset_test.df\n",
    "print(len(df))\n",
    "print(len(df[df[\"contact\"] == 1 & df[\"Endzone_left_1\"].isnull()]))\n",
    "print(len(df[df[\"contact\"] == 1 & df[\"Sideline_left_1\"].isnull()]))\n",
    "print(len(df[df[\"contact\"] == 1 & df[\"Endzone_left_1\"].isnull() & df[\"Sideline_left_1\"].isnull()]))\n",
    "print(len(df[df[\"contact\"] == 1 & ~df[\"Endzone_left_1\"].isnull() & ~df[\"Sideline_left_1\"].isnull()]))\n",
    "print(336 + 249 - 147 + 1568)\n",
    "df[df[\"contact\"] == 1 & ~df[\"Endzone_left_1\"].isnull() & ~df[\"Sideline_left_1\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contact_id            58204_002864_31_42486_G\n",
       "game_play                        58204_002864\n",
       "datetime             2020-09-27T19:18:16.100Z\n",
       "step                                       31\n",
       "nfl_player_id_1                         42486\n",
       "nfl_player_id_2                             G\n",
       "contact                                     1\n",
       "x_position_1                            110.4\n",
       "y_position_1                            28.16\n",
       "speed_1                                  0.86\n",
       "distance_1                               0.09\n",
       "direction_1                            147.31\n",
       "orientation_1                          344.54\n",
       "acceleration_1                           1.45\n",
       "sa_1                                    -1.44\n",
       "x_position_2                              NaN\n",
       "y_position_2                              NaN\n",
       "speed_2                                   NaN\n",
       "distance_2                                NaN\n",
       "direction_2                               NaN\n",
       "orientation_2                             NaN\n",
       "acceleration_2                            NaN\n",
       "sa_2                                      NaN\n",
       "distance                                  NaN\n",
       "G_flug                                   True\n",
       "frame                                     486\n",
       "Endzone_height_1                    34.416667\n",
       "Sideline_height_1                   21.208333\n",
       "Endzone_left_1                     125.458333\n",
       "Sideline_left_1                    841.083333\n",
       "Endzone_top_1                      313.041667\n",
       "Sideline_top_1                     312.458333\n",
       "Endzone_width_1                     37.458333\n",
       "Sideline_width_1                    19.041667\n",
       "Endzone_height_2                          NaN\n",
       "Sideline_height_2                         NaN\n",
       "Endzone_left_2                            NaN\n",
       "Sideline_left_2                           NaN\n",
       "Endzone_top_2                             NaN\n",
       "Sideline_top_2                            NaN\n",
       "Endzone_width_2                           NaN\n",
       "Sideline_width_2                          NaN\n",
       "Name: 250, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = df.query(\"game_play == '58187_001383' and nfl_player_id_1 == '38696' and frame == 732\")\n",
    "len(q)\n",
    "df.iloc[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=torch.tensor([0.485, 0.456, 0.406])\n",
    "std=torch.tensor([0.229, 0.224, 0.225])\n",
    "for r, d in enumerate(dl):\n",
    "    print(r)\n",
    "    imgs, feature, label = d\n",
    "    print(imgs.size())\n",
    "    imgs1, imgs2 = torch.split(imgs, 1, dim=1)\n",
    "    print(imgs1.size(), imgs2.size())\n",
    "    batch_size = imgs1.size()[0]\n",
    "\n",
    "\n",
    "    num_imgs = int(imgs1.size()[0])\n",
    "    # f, axarr = plt.subplots(2, num_imgs, figsize=(100,30))\n",
    "    f, axarr = plt.subplots(4, num_imgs // 2, figsize=(100, 30))\n",
    "    for i in range(num_imgs // 2):\n",
    "        img1, img2 = imgs1[i], imgs2[i]\n",
    "        axarr[0,i].imshow(img1[0].permute(1,2,0)*std+mean)  # end\n",
    "        axarr[1,i].imshow(img2[0].permute(1,2,0)*std+mean)  # side\n",
    "    for i in range(num_imgs // 2):\n",
    "        img1, img2 = imgs1[num_imgs // 2 + i], imgs2[num_imgs // 2 + i]\n",
    "        axarr[2,i].imshow(img1[0].permute(1,2,0)*std+mean)  # end\n",
    "        axarr[3,i].imshow(img2[0].permute(1,2,0)*std+mean)  # side\n",
    "        \n",
    "    plt.show()\n",
    "    if r >= 0:\n",
    "        break\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = cv2.imread(\"/workspace/data/processed/frames/58204_002864_Endzone.mp4_0390.jpg\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3373, 0.4431, 0.2275],\n",
       "         [0.3373, 0.4431, 0.2275],\n",
       "         [0.3373, 0.4431, 0.2275],\n",
       "         ...,\n",
       "         [0.7765, 0.7608, 0.7137],\n",
       "         [0.7767, 0.7608, 0.7137],\n",
       "         [0.7843, 0.7608, 0.7137]],\n",
       "\n",
       "        [[0.3411, 0.4508, 0.2236],\n",
       "         [0.3411, 0.4471, 0.2311],\n",
       "         [0.3411, 0.4506, 0.2240],\n",
       "         ...,\n",
       "         [0.7803, 0.7646, 0.7175],\n",
       "         [0.7805, 0.7646, 0.7175],\n",
       "         [0.7881, 0.7646, 0.7175]],\n",
       "\n",
       "        [[0.3449, 0.4547, 0.2235],\n",
       "         [0.3449, 0.4546, 0.2275],\n",
       "         [0.3449, 0.4547, 0.2237],\n",
       "         ...,\n",
       "         [0.7804, 0.7647, 0.7176],\n",
       "         [0.7804, 0.7647, 0.7176],\n",
       "         [0.7807, 0.7647, 0.7176]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.2980, 0.4314, 0.1922],\n",
       "         [0.3057, 0.4276, 0.1922],\n",
       "         [0.3021, 0.4237, 0.1884],\n",
       "         ...,\n",
       "         [0.5649, 0.6276, 0.5257],\n",
       "         [0.5619, 0.6244, 0.5225],\n",
       "         [0.5881, 0.6430, 0.5449]],\n",
       "\n",
       "        [[0.2980, 0.4314, 0.1922],\n",
       "         [0.3057, 0.4276, 0.1922],\n",
       "         [0.3022, 0.4238, 0.1885],\n",
       "         ...,\n",
       "         [0.5722, 0.6350, 0.5330],\n",
       "         [0.5681, 0.6306, 0.5288],\n",
       "         [0.5835, 0.6384, 0.5403]],\n",
       "\n",
       "        [[0.2980, 0.4314, 0.1922],\n",
       "         [0.3057, 0.4276, 0.1922],\n",
       "         [0.3059, 0.4275, 0.1922],\n",
       "         ...,\n",
       "         [0.5671, 0.6298, 0.5278],\n",
       "         [0.5339, 0.5964, 0.4945],\n",
       "         [0.5529, 0.6078, 0.5098]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=torch.tensor([0.485, 0.456, 0.406])\n",
    "std=torch.tensor([0.229, 0.224, 0.225])\n",
    "img1[0].permute(1,2,0)*std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
